{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gewünschte Lösung siehe unten - fehlende Unterstützung von Wordnet für deutsche Sprache. Abweichende Implementierung mit openthesaurus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "# Sicherstellen, dass alle erforderlichen NLTK-Resourcen geladen sind\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Laden des SpaCy-Modells\n",
    "nlp = spacy.load(\"de_core_news_sm\")  # Deutsches Modell verwenden, falls der Text deutsch ist\n",
    "\n",
    "file_path = \"v2_1.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "normalized_texts = []\n",
    "\n",
    "def get_synonym(word):\n",
    "    \"\"\"Findet ein Synonym für ein gegebenes Wort mit WordNet.\"\"\"\n",
    "    synonyms = wordnet.synsets(word, lang='de')  # 'lang' für Deutsch anpassen\n",
    "    if synonyms:\n",
    "        # Nimmt das erste Synonym\n",
    "        return synonyms[0].lemmas(lang='de')[0].name()\n",
    "    return word\n",
    "\n",
    "for text in tqdm(df['text'], desc=\"Verarbeitung der Reden\"):\n",
    "    tokens = text.split()  # Annahme: Der Text ist bereits vorverarbeitet und tokenisiert\n",
    "    synonym_replaced_tokens = [\n",
    "        get_synonym(token) for token in tokens\n",
    "    ]\n",
    "    normalized_texts.append(\" \".join(synonym_replaced_tokens))\n",
    "\n",
    "# Die 'text'-Spalte mit den normalisierten Texten überschreiben\n",
    "df['text'] = normalized_texts\n",
    "\n",
    "output_path = \"v2_2.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Normalisierte Daten mit Synonymen gespeichert unter: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schwierige Formattierung -- txt nicht wirklich verwertbar - Verusch der Synonym entfernung durch sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der entfernten Synonyme: 85692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verarbeitung der Reden: 100%|██████████| 34428/34428 [00:18<00:00, 1880.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der ersetzten Wörter: 8934148\n",
      "Normalisierte Daten mit Synonymen gespeichert unter: v2_2_2_synonym.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# Laden des SpaCy-Modells\n",
    "nlp = spacy.load(\"de_core_news_sm\")  # Deutsches Modell verwenden\n",
    "\n",
    "file_path = \"v2_1.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Funktion zum Parsen der OpenThesaurus-Daten\n",
    "def load_synonym_dictionary(file_path):\n",
    "    \"\"\"Parst die OpenThesaurus-Datenbank und erstellt ein Synonym-Wörterbuch.\"\"\"\n",
    "    synonym_dict = {}\n",
    "    removed_synonyms_count = 0\n",
    "    with open(file_path, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line.strip() and not line.startswith(\"#\"):\n",
    "                # Entfernt alles, was in Klammern steht\n",
    "                removed_synonyms_count += len(re.findall(r'\\([^)]*\\)', line))\n",
    "                line = re.sub(r'\\([^)]*\\)', '', line)\n",
    "                # Entfernt \"...\" aus der Zeile\n",
    "                line = re.sub(r'\\.\\.\\.', '', line)\n",
    "                words = [w.strip() for w in line.split(\";\")]\n",
    "                \n",
    "                # Priorisierung: Hauptform an den Anfang setzen\n",
    "                main_form = None\n",
    "                for word in words:\n",
    "                    if \"Hauptform\" in word:  # Erkennen der Hauptform\n",
    "                        main_form = word.replace(\"Hauptform\", \"\").strip()\n",
    "                        break\n",
    "                if main_form:\n",
    "                    # Hauptform an den Anfang der Liste setzen\n",
    "                    words = [main_form] + [w for w in words if w != main_form]\n",
    "                \n",
    "                for word in words:\n",
    "                    synonym_dict[word] = [w.strip() for w in words]  # Synonyme bereinigen\n",
    "    print(f\"Anzahl der entfernten Synonyme: {removed_synonyms_count}\")\n",
    "    return synonym_dict\n",
    "\n",
    "# Pfad zur heruntergeladenen OpenThesaurus-Datei\n",
    "thesaurus_path = \"openthesaurus.txt\"\n",
    "synonym_dict = load_synonym_dictionary(thesaurus_path)\n",
    "\n",
    "# Funktion zur Synonym-Ersetzung\n",
    "def get_synonym(word):\n",
    "    \"\"\"Findet ein Synonym aus dem Synonym-Wörterbuch.\"\"\"\n",
    "    synonyms = synonym_dict.get(word, [word])  # Falls kein Synonym vorhanden, Rückgabe des Wortes selbst\n",
    "    return synonyms[0]  # Erster Eintrag als Ersatz\n",
    "\n",
    "normalized_texts = []\n",
    "total_replacements = 0  # Zähler für ersetzte Wörter\n",
    "\n",
    "for text in tqdm(df['text'], desc=\"Verarbeitung der Reden\"):\n",
    "    tokens = text.split()  # Annahme: Der Text ist bereits vorverarbeitet und tokenisiert\n",
    "    synonym_replaced_tokens = []\n",
    "    for token in tokens:\n",
    "        clean_token = re.sub(r'[\\W_]', '', token)  # Entfernt Sonderzeichen aus jedem Token\n",
    "        if clean_token:  # Nur nicht-leere Tokens weiterverarbeiten\n",
    "            replacement = get_synonym(clean_token)\n",
    "            if replacement != clean_token:\n",
    "                total_replacements += 1\n",
    "            synonym_replaced_tokens.append(replacement)\n",
    "        else:\n",
    "            synonym_replaced_tokens.append(token)  # Belässt ursprüngliches Token bei Fehlern\n",
    "    normalized_texts.append(\" \".join(synonym_replaced_tokens))\n",
    "\n",
    "# Ausgabe der Anzahl der ersetzten Wörter\n",
    "print(f\"Anzahl der ersetzten Wörter: {total_replacements}\")\n",
    "\n",
    "# Die 'text'-Spalte mit den normalisierten Texten überschreiben\n",
    "df['text'] = normalized_texts\n",
    "\n",
    "output_path = \"v2_2_2_synonym.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Normalisierte Daten mit Synonymen gespeichert unter: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verarbeitung der Reden: 100%|██████████| 34428/34428 [58:21<00:00,  9.83it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der ersetzten Wörter: 0\n",
      "Normalisierte Daten mit Synonymen gespeichert unter: v2_2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import re\n",
    "import mysql.connector\n",
    "\n",
    "# Laden des SpaCy-Modells\n",
    "nlp = spacy.load(\"de_core_news_sm\")  # Deutsches Modell verwenden\n",
    "\n",
    "# Verbindung zur MySQL-Datenbank herstellen\n",
    "db_connection = mysql.connector.connect(\n",
    "    host=\"localhost\",       # MySQL-Host\n",
    "    user=\"root\",            # MySQL-Benutzername\n",
    "    password=\"admin\", # MySQL-Passwort\n",
    "    database=\"openthesaurus\" # Name der Datenbank\n",
    ")\n",
    "cursor = db_connection.cursor()\n",
    "\n",
    "# Funktion zur Abfrage des bevorzugten Synonyms aus der Datenbank\n",
    "def get_synonym_from_db(word):\n",
    "    \"\"\"Findet das bevorzugte Synonym für ein Wort in der MySQL-Datenbank.\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT t2.word\n",
    "    FROM term t1\n",
    "    JOIN term t2 ON t1.synset_id = t2.synset_id\n",
    "    JOIN synset s ON t1.synset_id = s.id\n",
    "    WHERE t1.word = %s AND t2.word = s.synset_preferred_term;\n",
    "    \"\"\"\n",
    "    cursor.execute(query, (word,))\n",
    "    result = cursor.fetchone()\n",
    "    return result[0] if result else word  # Rückgabe des bevorzugten Synonyms oder des Originalworts\n",
    "\n",
    "file_path = \"v2_1.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "normalized_texts = []\n",
    "total_replacements = 0  # Zähler für ersetzte Wörter\n",
    "\n",
    "for text in tqdm(df['text'], desc=\"Verarbeitung der Reden\"):\n",
    "    tokens = text.split()  # Annahme: Der Text ist bereits vorverarbeitet und tokenisiert\n",
    "    synonym_replaced_tokens = []\n",
    "    for token in tokens:\n",
    "        clean_token = re.sub(r'[\\W_]', '', token)  # Entfernt Sonderzeichen aus jedem Token\n",
    "        if clean_token:  # Nur nicht-leere Tokens weiterverarbeiten\n",
    "            replacement = get_synonym_from_db(clean_token)\n",
    "            if replacement != clean_token:\n",
    "                total_replacements += 1\n",
    "            synonym_replaced_tokens.append(replacement)\n",
    "        else:\n",
    "            synonym_replaced_tokens.append(token)  # Belässt ursprüngliches Token bei Fehlern\n",
    "    normalized_texts.append(\" \".join(synonym_replaced_tokens))\n",
    "\n",
    "# Ausgabe der Anzahl der ersetzten Wörter\n",
    "print(f\"Anzahl der ersetzten Wörter: {total_replacements}\")\n",
    "\n",
    "# Die 'text'-Spalte mit den normalisierten Texten überschreiben\n",
    "df['text'] = normalized_texts\n",
    "\n",
    "output_path = \"v2_2.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Normalisierte Daten mit Synonymen gespeichert unter: {output_path}\")\n",
    "\n",
    "# Datenbankverbindung schließen\n",
    "cursor.close()\n",
    "db_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vergleich der Datein - mit Synonymen und Ohne   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words (Original): 7427891\n",
      "Total Words (Cleaned): 7591942\n",
      "Unique Words (Original): 219655\n",
      "Unique Words (Cleaned): 228105\n",
      "Removed Synonyms (Count): 14141\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Hilfsfunktion zur Bereinigung von Text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Umwandlung in Kleinbuchstaben\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Entfernen von Satzzeichen\n",
    "    return text\n",
    "\n",
    "# Funktion zur Analyse der Textunterschiede\n",
    "def analyze_text_changes(file_original, file_cleaned, text_column):\n",
    "    # CSV-Dateien laden\n",
    "    df_original = pd.read_csv(file_original)\n",
    "    df_cleaned = pd.read_csv(file_cleaned)\n",
    "\n",
    "    # Textspalten extrahieren\n",
    "    texts_original = df_original[text_column].dropna().apply(preprocess_text)\n",
    "    texts_cleaned = df_cleaned[text_column].dropna().apply(preprocess_text)\n",
    "\n",
    "    # Wortlisten erstellen\n",
    "    words_original = Counter(\" \".join(texts_original).split())\n",
    "    words_cleaned = Counter(\" \".join(texts_cleaned).split())\n",
    "\n",
    "    # Einzigartige Wörter zählen\n",
    "    unique_words_original = set(words_original.keys())\n",
    "    unique_words_cleaned = set(words_cleaned.keys())\n",
    "\n",
    "    # Vergleich der Wortanzahl\n",
    "    total_words_original = sum(words_original.values())\n",
    "    total_words_cleaned = sum(words_cleaned.values())\n",
    "\n",
    "    # Synonyme (entfernte Wörter)\n",
    "    removed_words = unique_words_original - unique_words_cleaned\n",
    "\n",
    "    # Ergebnisse ausgeben\n",
    "    results = {\n",
    "        \"Total Words (Original)\": total_words_original,\n",
    "        \"Total Words (Cleaned)\": total_words_cleaned,\n",
    "        \"Unique Words (Original)\": len(unique_words_original),\n",
    "        \"Unique Words (Cleaned)\": len(unique_words_cleaned),\n",
    "        \"Removed Synonyms (Count)\": len(removed_words),\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "# Dateien und Spaltennamen\n",
    "file_original = \"v2_2_stoppword_removal_wo_synonym_replacement.csv\"\n",
    "file_cleaned = \"v2_2_stoppword_removal_w_synonym_replacement.csv\"\n",
    "text_column = \"text\"  # Ersetzen Sie dies mit dem Namen der Textspalte\n",
    "\n",
    "# Analyse durchführen\n",
    "results = analyze_text_changes(file_original, file_cleaned, text_column)\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: Synonym Replacements\n",
    "\n",
    "Total Words (Original): 18526649\n",
    "Total Words (Cleaned): 18526649\n",
    "Unique Words (Original): 288958\n",
    "Unique Words (Cleaned): 268188\n",
    "Removed Synonyms (Count): 24835\n",
    "\n",
    "8,6% less unique entrys. \n",
    "\n",
    "Results - Stoppwordremoval, Lemmantizierung after Synonym Replacements\n",
    "Total Words (Original): 7427891\n",
    "Total Words (Cleaned): 7591942\n",
    "Unique Words (Original): 219655\n",
    "Unique Words (Cleaned): 228105\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
